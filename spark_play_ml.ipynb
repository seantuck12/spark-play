{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Play 02: First ML algorithms with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of `pyspark.ml` are the `Transformer` and `Estimator` classes.\n",
    "\n",
    "`Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame with columns added or existing columns transformed. Examples are the `Bucketizer` class for binning continuous data, or the `PCA` class for creating principal components.\n",
    "\n",
    "`Estimator` classes have a `.fit()` method that takes a DataFrame and returns a model object. Examples are the `RandomForestModel` class, for fitting RF classification or regression, or the `StringIndexerModel` class, for including categorical data saved a string in your model (like factors in R?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline # Object which lets us access all pyspark.ml's features in chained operations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.csv(\"./data/dc/flights.csv\", inferSchema=True, header=True)\n",
    "planes = spark.read.csv(\"./data/dc/planes.csv\", inferSchema=True, header=True)\n",
    "airports = spark.read.csv(\"./data/dc/airports.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict whether a flight will be delayed (classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|                type|    manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "| N102UW|1998|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N103US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N104UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N105UW|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N107US|1999|Fixed wing multi ...|AIRBUS INDUSTRIE|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "+-------+----+--------------------+----------------+--------+-------+-----+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\") # flights has year column so kep this distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark models require all data in numeric form. You can modify columns using `withColumn(colName, col)` as we've seen, but within that call you can use the `cast(dataType)` method on a column object to coerce its data to a different type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can engineer new columns using `withColumn()` too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our response/target variable too: Was the flight delayed? Code as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\")) # label is Spark's default name for response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I named the response variable `label` because that is Spark's default name for it in its ML routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove missing values using SQL syntax like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `StringIndexer` class to create integer variables from categorical data. There are several steps to this:\n",
    "1. `StringIndexer` is an `Estimator` class that takes a DataFrame and maps numbers to unique values of its categorical variable\n",
    "2. The `Estimator` returns a `Transformer` that takes a DataFrame and returns the DataFrame with an appended column of the mapped numbers\n",
    "3. This new numeric variable can then be encoding as a one-hot vector using a `OneHotEncoder`. This works like the `StringIndexer`, by creating an `Estimator` followed by a `Transformer` to first _map_ the data to new encodings and then _apply_ those new encodings to the data.\n",
    "\n",
    "Basically, you need a `StringIndexer` and then a `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the carrier categorical variable as an example. In each step supply the input column name and a name for the new output column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "carrier_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_indexer = StringIndexer(inputCol=\"dest\", outputCol=\"dest_index\")\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\", outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Spark ML routine expects your explanatory variables to combined into **one column** so that each observation has one target variable column (`label`) and one explanatory variable column (`features`) which contains a vector of explanatory variables for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this step happen, we can use the `VectorAssembler` transformer method, which takes a list of input columns and the new for the output column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=['month', 'air_time', 'carrier_fact', 'dest_fact', 'plane_age'],\n",
    "                                outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to wrap up all the transformation and feature engineering steps, plus the vector assembly, into one `Pipeline` object which lets Spark handle all the detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(stages=[carrier_indexer, carrier_encoder,\n",
    "                              dest_indexer, dest_encoder,\n",
    "                              vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your `Pipeline` sorted, you can pass your data through it to get your piped data, using `Pipeline`'s `.fit(dataset)` and `.transform(dataset)` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data = model_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and train splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Always make train/test splits after all transformations**, because `StringIndexer` may give different indices even when given same list of strings. We can use the `randomSplit(weights, seed=None)` method on our piped data, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = piped_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-play",
   "language": "python",
   "name": "spark-play"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
