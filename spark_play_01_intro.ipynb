{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Play 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup instructions for running pyspark in jupyter notebooks (using conda environment, such as \"my-env\"):\n",
    "```\n",
    "java -version # Should be 1.8.0_241 ... i.e. Final Java 8 release \n",
    "conda create -n my-env\n",
    "conda activate -n my-env python=3.7 jupyter\n",
    "conda install -c conda-forge pyspark\n",
    "conda install -c anaconda ipykernel\n",
    "python -m ipykernel install --user --name=my-env\n",
    "jupyter notebook\n",
    "```\n",
    "Open new notebook file by selecting New > my-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as psf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a `SparkContext` which connects the Spark application to the cluster (in this case local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local[*]')\n",
    "print(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get or create a new `SparkSession` which we'll use to control the Spark driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001F8A5DE5A88>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the contents of the Spark session using its `catalog` attribute, which has methods such as `listTables()`. (Currently nothing in the session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful `catalog` methods might include:\n",
    "- `cacheTable(tableName)`/`uncacheTable(tableName)`/`isCached(tableName)`\n",
    "- `createTable()`\n",
    "- `listColumns(tableName)`\n",
    "- `listFunctions(dbName=None)` -- functions registered in a specified database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spark driver for first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start off generating sequences to play with using the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000) # Returns dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000).toDF(\"number\") # Give it column header 'number'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could assign that dataframe to a variable and work on that. Or we can keep chaining functions, using functional programming style, to minimise use of intermediate global objects and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000)\\\n",
    "    .toDF(\"number\")\\\n",
    "    .where(\"number % 2 = 0\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executions we've run above are saved in the Spark application as previous \"jobs\". You can view the job history, among other things, by looking at the Spark UI, on port 4040 of the driver node, which in our local case is http://localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running first queries on a dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run SQL queries or dataframe manipulations through the `SparkSession`. But first we need to read some data into the session, using its `read` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.csv(\"./data/flights.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a dataframe manipulation chain, finding the number of flights within each origin-destination group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='LAX', total_flights=13744),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='SFO', total_flights=13457),\n",
       " Row(ORIGIN_AIRPORT='JFK', DESTINATION_AIRPORT='LAX', total_flights=12016),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='JFK', total_flights=12015),\n",
       " Row(ORIGIN_AIRPORT='LAS', DESTINATION_AIRPORT='LAX', total_flights=9715),\n",
       " Row(ORIGIN_AIRPORT='LGA', DESTINATION_AIRPORT='ORD', total_flights=9639),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='LAS', total_flights=9594),\n",
       " Row(ORIGIN_AIRPORT='ORD', DESTINATION_AIRPORT='LGA', total_flights=9575),\n",
       " Row(ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='JFK', total_flights=8440),\n",
       " Row(ORIGIN_AIRPORT='JFK', DESTINATION_AIRPORT='SFO', total_flights=8437)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights\\\n",
    "    .select('FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "        'SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL',\n",
    "        'DEPARTURE_TIME', 'ARRIVAL_TIME', 'ARRIVAL_DELAY')\\\n",
    "    .groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed('count', 'total_flights')\\\n",
    "    .sort(psf.desc('total_flights'))\\\n",
    "    .take(10) # For comparison with sql code, could be written as .limit(10)/.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent can be written as a SQL query, and from Spark's point of view they are an identical implementation. To run SQL queries we must first convert the dataframe into a database table. We then query the table by passing SQL code to the session driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(origin_airport='SFO', destination_airport='LAX', total_flights=13744),\n",
       " Row(origin_airport='LAX', destination_airport='SFO', total_flights=13457),\n",
       " Row(origin_airport='JFK', destination_airport='LAX', total_flights=12016),\n",
       " Row(origin_airport='LAX', destination_airport='JFK', total_flights=12015),\n",
       " Row(origin_airport='LAS', destination_airport='LAX', total_flights=9715),\n",
       " Row(origin_airport='LGA', destination_airport='ORD', total_flights=9639),\n",
       " Row(origin_airport='LAX', destination_airport='LAS', total_flights=9594),\n",
       " Row(origin_airport='ORD', destination_airport='LGA', total_flights=9575),\n",
       " Row(origin_airport='SFO', destination_airport='JFK', total_flights=8440),\n",
       " Row(origin_airport='JFK', destination_airport='SFO', total_flights=8437)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    FROM flights\n",
    "        SELECT origin_airport, destination_airport, count(*) AS total_flights\n",
    "        GROUP BY origin_airport, destination_airport\n",
    "        ORDER BY total_flights DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You don't have to cap off SQL queries with `;`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note also:** The `take()` and `collect()` methods are actions at the end of spark queries which will **load the output data into the driver's memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the queried data using either SQL or DataFrame styles, and if we create a local `pandas` copy using `.toPandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>avg_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IAD</td>\n",
       "      <td>TTN</td>\n",
       "      <td>381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWF</td>\n",
       "      <td>PBI</td>\n",
       "      <td>260.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIC</td>\n",
       "      <td>CAE</td>\n",
       "      <td>228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RDU</td>\n",
       "      <td>IND</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10581</td>\n",
       "      <td>11618</td>\n",
       "      <td>163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FCA</td>\n",
       "      <td>MSO</td>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SWF</td>\n",
       "      <td>RSW</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10581</td>\n",
       "      <td>12953</td>\n",
       "      <td>138.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14843</td>\n",
       "      <td>12264</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OAK</td>\n",
       "      <td>FLL</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11433</td>\n",
       "      <td>11423</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11433</td>\n",
       "      <td>13367</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11423</td>\n",
       "      <td>11433</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14222</td>\n",
       "      <td>12173</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10423</td>\n",
       "      <td>13487</td>\n",
       "      <td>90.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OKC</td>\n",
       "      <td>FLL</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JAC</td>\n",
       "      <td>JFK</td>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEN</td>\n",
       "      <td>PIA</td>\n",
       "      <td>84.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TTN</td>\n",
       "      <td>CVG</td>\n",
       "      <td>75.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MDT</td>\n",
       "      <td>DEN</td>\n",
       "      <td>72.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ORIGIN_AIRPORT DESTINATION_AIRPORT   avg_delay\n",
       "0             IAD                 TTN  381.000000\n",
       "1             SWF                 PBI  260.500000\n",
       "2             RIC                 CAE  228.000000\n",
       "3             RDU                 IND  208.000000\n",
       "4           10581               11618  163.000000\n",
       "5             FCA                 MSO  148.000000\n",
       "6             SWF                 RSW  140.000000\n",
       "7           10581               12953  138.333333\n",
       "8           14843               12264  122.000000\n",
       "9             OAK                 FLL  106.000000\n",
       "10          11433               11423  106.000000\n",
       "11          11433               13367  101.000000\n",
       "12          11423               11433  100.000000\n",
       "13          14222               12173  100.000000\n",
       "14          10423               13487   90.142857\n",
       "15            OKC                 FLL   90.000000\n",
       "16            JAC                 JFK   89.000000\n",
       "17            DEN                 PIA   84.750000\n",
       "18            TTN                 CVG   75.666667\n",
       "19            MDT                 DEN   72.500000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights\\\n",
    "    .select('FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "        'SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL',\n",
    "        'DEPARTURE_TIME', 'ARRIVAL_TIME', 'ARRIVAL_DELAY')\\\n",
    "    .groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\\\n",
    "    .avg('ARRIVAL_DELAY')\\\n",
    "    .withColumnRenamed('avg(ARRIVAL_DELAY)', 'avg_delay')\\\n",
    "    .sort(psf.desc('avg_delay'))\\\n",
    "    .limit(20)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more for luck..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_airport</th>\n",
       "      <th>destination_airport</th>\n",
       "      <th>airline</th>\n",
       "      <th>duration_hrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JFK</td>\n",
       "      <td>HNL</td>\n",
       "      <td>DL</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JFK</td>\n",
       "      <td>HNL</td>\n",
       "      <td>HA</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12478</td>\n",
       "      <td>12173</td>\n",
       "      <td>HA</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EWR</td>\n",
       "      <td>HNL</td>\n",
       "      <td>UA</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11618</td>\n",
       "      <td>12173</td>\n",
       "      <td>UA</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14735</th>\n",
       "      <td>CWA</td>\n",
       "      <td>ATW</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14736</th>\n",
       "      <td>MCO</td>\n",
       "      <td>HPN</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14737</th>\n",
       "      <td>GCC</td>\n",
       "      <td>RAP</td>\n",
       "      <td>OO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14738</th>\n",
       "      <td>FSD</td>\n",
       "      <td>LNK</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14739</th>\n",
       "      <td>SWF</td>\n",
       "      <td>LGA</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14740 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      origin_airport destination_airport airline  duration_hrs\n",
       "0                JFK                 HNL      DL         10.74\n",
       "1                JFK                 HNL      HA         10.58\n",
       "2              12478               12173      HA         10.46\n",
       "3                EWR                 HNL      UA         10.21\n",
       "4              11618               12173      UA         10.02\n",
       "...              ...                 ...     ...           ...\n",
       "14735            CWA                 ATW      EV           NaN\n",
       "14736            MCO                 HPN      EV           NaN\n",
       "14737            GCC                 RAP      OO           NaN\n",
       "14738            FSD                 LNK      EV           NaN\n",
       "14739            SWF                 LGA      EV           NaN\n",
       "\n",
       "[14740 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin_airport, destination_airport, airline, ROUND(AVG(air_time) / 60, 2) AS duration_hrs\n",
    "FROM flights\n",
    "GROUP BY origin_airport, destination_airport, airline\n",
    "ORDER BY duration_hrs DESC\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on immutability in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames are immutable, because you're supposed to leave the data as-is and do all the transformations in your code. If you wanted to update your DataFrame, you'd have to re-assign it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we needed to add an extra column. We could use the Spark DataFrame's `withColumn()` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = flights.withColumn('duration_hrs', flights.AIR_TIME / 60)\n",
    "'duration_hrs' in flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter()` method takes on of two options:\n",
    "- A string, which represents what would follow a WHERE clause in SQL, or\n",
    "- A Spark Column of boolean (True/False) values\n",
    "\n",
    "In other words, the following two lines are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, AIRLINE: string, FLIGHT_NUMBER: int, TAIL_NUMBER: string, ORIGIN_AIRPORT: string, DESTINATION_AIRPORT: string, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, TAXI_OUT: int, WHEELS_OFF: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, AIR_TIME: int, DISTANCE: int, WHEELS_ON: int, TAXI_IN: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, CANCELLATION_REASON: string, AIR_SYSTEM_DELAY: int, SECURITY_DELAY: int, AIRLINE_DELAY: int, LATE_AIRCRAFT_DELAY: int, WEATHER_DELAY: int, duration_hrs: double] \n",
      "\n",
      "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, AIRLINE: string, FLIGHT_NUMBER: int, TAIL_NUMBER: string, ORIGIN_AIRPORT: string, DESTINATION_AIRPORT: string, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, TAXI_OUT: int, WHEELS_OFF: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, AIR_TIME: int, DISTANCE: int, WHEELS_ON: int, TAXI_IN: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, CANCELLATION_REASON: string, AIR_SYSTEM_DELAY: int, SECURITY_DELAY: int, AIRLINE_DELAY: int, LATE_AIRCRAFT_DELAY: int, WEATHER_DELAY: int, duration_hrs: double]\n"
     ]
    }
   ],
   "source": [
    "print(flights.filter(\"air_time > 120\"), \"\\n\")\n",
    "print(flights.filter(flights.AIR_TIME > 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** That when using string approach, the variable names are **not case sensitive**, just like with SQL queries. But when writing in Python code, they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select()` method takes multiple arguments that define which columns to select, either as strings of column names or the column objects themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, DEPARTURE_TIME: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"arrival_time\", flights.DEPARTURE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note again:** That when selecting columns using strings, they are not case sensitive, just like in SQL. But passing a column object is case sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw just a moment above, you can perform operations on your columns to transform them using the `withColumn()` method, which returns all columns in the outputted DataFrame.\n",
    "\n",
    "But you can also do this inside `select()`, one of two ways:\n",
    "- Pass it a column object which we transform, with the option of renaming using the `alias()` method.\n",
    "- Pass it strings, which represent SQL expressions we'd pass to `SELECT`, with the option of renaming using `AS`. **But to do it this way, you have to use `selectExpr()` instead of `select()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, DEPARTURE_TIME: int, duration_hrs: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"arrival_time\", flights.DEPARTURE_TIME, (flights.AIR_TIME / 60).alias('duration_hrs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, departure_time: int, duration_hrs: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.selectExpr(\"arrival_time\", \"departure_time\", \"air_time / 60 AS duration_hrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[origin_airport: string, destination_airport: string, tail_number: string, avg_speed: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"origin_airport\", \"destination_airport\", \"tail_number\",\n",
    "               (flights.DISTANCE/(flights.AIR_TIME/60)).alias(\"avg_speed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[origin_airport: string, destination_airport: string, tail_number: string, avg_speed: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.selectExpr(\"origin_airport\", \"destination_airport\", \"tail_number\",\n",
    "                   \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group our DataFrame using `groupBy()`. Once we've created a grouped DataFrame (GroupedData), we can use the GroupedDat methods for aggregation, like `min()`, `max()`, `count()`, `avg()`, and so on. We can aggregate the whole data by leaving `groupBy()` empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average flight duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| avg(duration_hrs)|\n",
      "+------------------+\n",
      "|1.8918604681687405|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .select((flights.AIR_TIME / 60).alias('duration_hrs'))\\\n",
    "    .groupBy()\\\n",
    "    .avg('duration_hrs')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average flight duration by airline carrier, sorted in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|airline| avg(duration_hrs)|\n",
      "+-------+------------------+\n",
      "|     VX| 3.043846601793797|\n",
      "|     UA|2.7472647618372408|\n",
      "|     AS|2.6307464073713316|\n",
      "|     B6| 2.397641535835225|\n",
      "|     AA|2.3301034689931734|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .selectExpr(\"airline\", \"air_time / 60 AS duration_hrs\")\\\n",
    "    .groupBy('airline')\\\n",
    "    .avg('duration_hrs')\\\n",
    "    .sort(psf.desc('avg(duration_hrs)'))\\\n",
    "    .show(5)\n",
    "\n",
    "# ^^ Note: See agg() below for renaming var at same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of flights taken, by airline and origin airport, sorted in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+\n",
      "|origin_airport|airline| count|\n",
      "+--------------+-------+------+\n",
      "|           ATL|     DL|221705|\n",
      "|           DFW|     AA|134270|\n",
      "|           MDW|     WN| 76350|\n",
      "|           LAS|     WN| 68520|\n",
      "|           BWI|     WN| 64063|\n",
      "+--------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .groupBy('origin_airport', 'airline')\\\n",
    "    .count()\\\n",
    "    .sort(psf.desc('count'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `agg()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the basic aggregation functions, there is also the generic `agg()` method, which lets us pass any of the aggregation functions from the `pyspark.sql.functions` submodule. (I loaded this submodule earlier as `psf`.)\n",
    "\n",
    "Every function in the submodule takes the name of a GroupedData column as its argument.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------------+\n",
      "|month|destination_airport|   dep_delay_stdev|\n",
      "+-----+-------------------+------------------+\n",
      "|    1|                ACY| 32.81265545745788|\n",
      "|    1|                EYW| 63.65052137951282|\n",
      "|    1|                OME|19.546923118225806|\n",
      "|    1|                RDM|34.260918828983726|\n",
      "|    1|                TWF| 7.985009333135624|\n",
      "+-----+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .groupBy('month', 'destination_airport')\\\n",
    "    .agg((psf.stddev('departure_delay')).alias('dep_delay_stdev'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can use the `alias()` method with aggregation as well as when selecting columns as we saw earlier. But I don't think aliasing works when using the basic aggregators; you have to wrap it inside `agg()` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "+---------+--------------------+\n",
      "|       UA|United Air Lines ...|\n",
      "|       AA|American Airlines...|\n",
      "|       US|     US Airways Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "|       B6|     JetBlue Airways|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = spark.read.csv(\"./data/airlines.csv\", inferSchema=True, header=True)\n",
    "airlines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|       CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|      ABE|Lehigh Valley Int...|  Allentown|   PA|    USA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|    Abilene|   TX|    USA|32.41132|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|Albuquerque|   NM|    USA|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|   Aberdeen|   SD|    USA|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|     Albany|   GA|    USA|31.53552| -84.19447|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read.csv(\"./data/airports.csv\", inferSchema=True, header=True)\n",
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `join()` method lets us do all kinds of joins. First arg is the dataset we want to use in our join, second arg `on` specifies the keys to join on, and third arg `how` specifies which kind of join to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "|iata_code|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|     duration_hrs|             AIRPORT|           CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "|      SEA|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                  5|          2354|            -11|      21|        15|           205|         194|     169|    1448|      404|      4|              430|         408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|2.816666666666667|Seattle-Tacoma In...|        Seattle|   WA|    USA|47.44898|-122.30931|\n",
      "|      PBI|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                 10|             2|             -8|      12|        14|           280|         279|     263|    2330|      737|      4|              750|         741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|4.383333333333334|Palm Beach Intern...|West Palm Beach|   FL|    USA|26.68316| -80.09559|\n",
      "|      CLT|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                 20|            18|             -2|      16|        34|           286|         293|     266|    2296|      800|     11|              806|         811|            5|       0|        0|               null|            null|          null|         null|               null|         null|4.433333333333334|Charlotte Douglas...|      Charlotte|   NC|    USA|35.21401| -80.94313|\n",
      "|      MIA|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                 20|            15|             -5|      15|        30|           285|         281|     258|    2342|      748|      8|              805|         756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|              4.3|Miami Internation...|          Miami|   FL|    USA|25.79325| -80.29056|\n",
      "|      ANC|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                 25|            24|             -1|      11|        35|           235|         215|     199|    1448|      254|      5|              320|         259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|3.316666666666667|Ted Stevens Ancho...|      Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .withColumnRenamed(\"destination_airport\", \"iata_code\")\\\n",
    "    .join(airports, on=\"iata_code\", how=\"leftouter\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-play",
   "language": "python",
   "name": "spark-play"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
