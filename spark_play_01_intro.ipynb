{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Play 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup instructions for running pyspark in jupyter notebooks (using conda environment, such as \"my-env\"):\n",
    "```\n",
    "java -version # Should be 1.8.0_241 ... i.e. Final Java 8 release \n",
    "conda create -n my-env\n",
    "conda activate -n my-env python=3.7 jupyter\n",
    "conda install -c conda-forge pyspark\n",
    "conda install -c anaconda ipykernel\n",
    "python -m ipykernel install --user --name=my-env\n",
    "jupyter notebook\n",
    "```\n",
    "Open new notebook file by selecting New > my-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as psf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a `SparkContext` which connects the Spark application to the cluster (in this case local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "2.4.5\n",
      "3.7\n",
      "local[*]\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local[*]') # Interacts with RDDs API\n",
    "print(sc)\n",
    "print(sc.version) # Shows version of Spark being used\n",
    "print(sc.pythonVer) # Shows version of Python being used\n",
    "print(sc.master) # Shows 'name' of master node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get or create a new `SparkSession` which we'll use to control the Spark driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000221591CED48>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate() # Interacts with DataFrames API, see later\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the contents of the Spark session using its `catalog` attribute, which has methods such as `listTables()`. (Currently nothing in the session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful `catalog` methods might include:\n",
    "- `cacheTable(tableName)`/`uncacheTable(tableName)`/`isCached(tableName)`\n",
    "- `createTable()`\n",
    "- `listColumns(tableName)`\n",
    "- `listFunctions(dbName=None)` -- functions registered in a specified database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load simple objects into pyspark by, for example, passing a number range into the SparkContext `sc` using its `parallelize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:195\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "first_range = sc.parallelize(np.arange(1, 101))\n",
    "print(first_range)\n",
    "print(type(first_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sc.parallelize()` creates an RDD (Resilient Distributed Datasets). We express our computations through Spark and using its distributed data structures it deals with the parallelisation automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load local files into Spark's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md MapPartitionsRDD[6] at textFile at <unknown>:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "first_txt = sc.textFile('README.md')\n",
    "print(first_txt)\n",
    "print(type(first_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD objects have a few attributes we can use explore the partitioning it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(first_txt.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the minimum number of partitions we want when we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "first_txt = sc.textFile('README.md', minPartitions=5)\n",
    "print(first_txt.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spark driver for first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start off generating sequences to play with using the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000) # Returns Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000).toDF(\"number\") # Give it column header 'number'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could assign that dataframe to a variable and work on that. Or we can keep chaining functions, using functional programming style, to minimise use of intermediate global objects and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000)\\\n",
    "    .toDF(\"number\")\\\n",
    "    .where(\"number % 2 = 0\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executions we've run above are saved in the Spark application as previous \"jobs\". You can view the job history, among other things, by looking at the Spark UI, on port 4040 of the driver node, which in our local case is http://localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How jobs work: Transformations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each executed operation in Spark, or job, consists of a series of transformations on the data structure, ending with an action which executes the full operation and (typically) loads the output into memory. Spark uses lazy evaluation to avoid loading the data until the very end, when large datasets can be reduced to filtered/aggregated sets.\n",
    "\n",
    "Transformation examples:\n",
    "- `.map(fun)`\n",
    "- `.filter(expression)`\n",
    "- `.filterByKey(expression)`\n",
    "- `.reduce(expression)`\n",
    "- `.reduceByKey(expression)`\n",
    "- `.sort(var)`\n",
    "- `.sortByKey(var)`\n",
    "\n",
    "Action examples:\n",
    "- `.collect()`\n",
    "- `.collectAsMap()` (returns pair RDD as dict)\n",
    "- `.take(n)`\n",
    "- `.count()`\n",
    "- `.countByKey()` (pair RDDs only, see later)\n",
    "- `.toPandas()`\n",
    "\n",
    "\n",
    "The `.saveAsTextFile(\"tempFile\")` action avoids loading into memory by instead saving to a file. By default, you specify a directory and the data are written to multiple files, one for each partition. `.coalesce(1).saveAsTextFile(\"tempFile\")` overrides this default and saves to one file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark's RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen Spark's RDDs above, like `first_txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# spark-play',\n",
       " 'Getting to know Apache Spark with pyspark',\n",
       " '---------------------------------------------',\n",
       " '',\n",
       " 'This code is intended to document my learning of Spark and pyspark, so I can return to this repository to remind myself who to use it.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_txt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RDD is basically a distributed collection of values. But we can use pair RDDs to have _distributed collections of key-value pairs_. This extra level of structure goes some way towards the creation of RDD Datasets that we'll see later. Pair RDDs allow transformations by key, such as `reduceByKey()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(f\"Key {num[0]} has {num[1]} Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the above output is not reproducible because the key ordering can differ, due to the distributed nature of its input. But you can use `sortByKey()` to make it reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "for num in Rdd_Reduced.sortByKey(ascending=False).collect():\n",
    "  print(f\"Key {num[0]} has {num[1]} Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2)]\n",
      "defaultdict(<class 'int'>, {1: 1, 3: 2, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Rdd.take(1))\n",
    "print(Rdd.countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{print(\"key\", k, \"has\", v, \"counts\") for k,v in Rdd.countByKey().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using RDD with complete works of Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most commonly used words in the complete works of William Shapespeare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill = sc.textFile(\"./data/shakespeare/complete_shakespeare.txt\", minPartitions=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shakespeares works are currently split into lines. Split into words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 128576\n"
     ]
    }
   ],
   "source": [
    "bill_words = bill.flatMap(lambda x: x.split()) # flatMap does map() and flattens the results.\n",
    "print(f\"Total words: {bill_words.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below will filter out stop words. Then it will create a pair RDD, where each word is a key and its value starts as 1 (its starting count). Then we'll reduce by key (unique word), counting the occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = bill_words\\\n",
    "    .filter(lambda x: x.lower() not in stop_words)\\\n",
    "    .map(lambda w: (w, 1))\\\n",
    "    .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 9),\n",
       " ('anywhere', 1),\n",
       " ('whatsoever.', 1),\n",
       " ('away', 30),\n",
       " ('copyright', 6),\n",
       " ('guidelines', 1),\n",
       " ('file.', 1),\n",
       " ('2011', 1),\n",
       " ('Language:', 1),\n",
       " ('GUTENBERG', 28)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to know the 10 most common words, so we need to sort by values. One way to do this is to actually swap the keys and values around, then sort by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(650, 'thou'),\n",
       " (574, 'thy'),\n",
       " (393, 'shall'),\n",
       " (311, 'would'),\n",
       " (295, 'good'),\n",
       " (286, 'thee'),\n",
       " (273, 'love'),\n",
       " (269, 'Enter'),\n",
       " (254, \"th'\"),\n",
       " (225, 'make')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts\\\n",
    "    .map(lambda x: (x[1], x[0]))\\\n",
    "    .sortByKey(ascending=False)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running first queries on a Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using SparkContext, called `sc` in spark-shell, as an entry point for interacting with RDDs, we now use the SparkSession, called `spark` in spark-shell, as the point of entry for interacting with Spark DataFrames.\n",
    "\n",
    "Spark DataFrames are data structures that use RDDs at the core, but provide SQL/dataframe functionality and create optimised queries.\n",
    "\n",
    "**It's recommended to interact with Spark using this newer DataFrames API rather than the older RDD API, so we'll almost always be using `spark` instead of `sc`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run SQL queries or dataframe manipulations through the `SparkSession`. But first we need to read some data into the session, using its `read` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.csv(\"./data/kaggle/flights.csv\", inferSchema=True, header=True) # We could pass the schema using schema arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we needed to load a .txt or .json file instead, we'd use `spark.read.txt()` or `spark.read.json()` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are `flights`'s dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5819079\n",
      "Columns: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {flights.count()}\\nColumns: {len(flights.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY',\n",
       " 'DAY_OF_WEEK',\n",
       " 'AIRLINE',\n",
       " 'FLIGHT_NUMBER',\n",
       " 'TAIL_NUMBER',\n",
       " 'ORIGIN_AIRPORT',\n",
       " 'DESTINATION_AIRPORT',\n",
       " 'SCHEDULED_DEPARTURE',\n",
       " 'DEPARTURE_TIME',\n",
       " 'DEPARTURE_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'SCHEDULED_TIME',\n",
       " 'ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'SCHEDULED_ARRIVAL',\n",
       " 'ARRIVAL_TIME',\n",
       " 'ARRIVAL_DELAY',\n",
       " 'DIVERTED',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_REASON',\n",
       " 'AIR_SYSTEM_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'AIRLINE_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'WEATHER_DELAY']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than just looking at column names, we can print the entire schema for the DataFrame like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of `flights`'s rows are unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.dropDuplicates().count() / flights.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a dataframe manipulation chain, finding the number of flights within each origin-destination group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='LAX', total_flights=13744),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='SFO', total_flights=13457),\n",
       " Row(ORIGIN_AIRPORT='JFK', DESTINATION_AIRPORT='LAX', total_flights=12016),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='JFK', total_flights=12015),\n",
       " Row(ORIGIN_AIRPORT='LAS', DESTINATION_AIRPORT='LAX', total_flights=9715),\n",
       " Row(ORIGIN_AIRPORT='LGA', DESTINATION_AIRPORT='ORD', total_flights=9639),\n",
       " Row(ORIGIN_AIRPORT='LAX', DESTINATION_AIRPORT='LAS', total_flights=9594),\n",
       " Row(ORIGIN_AIRPORT='ORD', DESTINATION_AIRPORT='LGA', total_flights=9575),\n",
       " Row(ORIGIN_AIRPORT='SFO', DESTINATION_AIRPORT='JFK', total_flights=8440),\n",
       " Row(ORIGIN_AIRPORT='JFK', DESTINATION_AIRPORT='SFO', total_flights=8437)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights\\\n",
    "    .select('FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "        'SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL',\n",
    "        'DEPARTURE_TIME', 'ARRIVAL_TIME', 'ARRIVAL_DELAY')\\\n",
    "    .groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed('count', 'total_flights')\\\n",
    "    .sort(psf.desc('total_flights'))\\\n",
    "    .take(10) # For comparison with sql code, could be written as .limit(10)/.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent can be written as a SQL query, and from Spark's point of view they are an identical implementation. To run SQL queries we must first convert the dataframe into a database table. We then query the table by passing SQL code to the session driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(origin_airport='SFO', destination_airport='LAX', total_flights=13744),\n",
       " Row(origin_airport='LAX', destination_airport='SFO', total_flights=13457),\n",
       " Row(origin_airport='JFK', destination_airport='LAX', total_flights=12016),\n",
       " Row(origin_airport='LAX', destination_airport='JFK', total_flights=12015),\n",
       " Row(origin_airport='LAS', destination_airport='LAX', total_flights=9715),\n",
       " Row(origin_airport='LGA', destination_airport='ORD', total_flights=9639),\n",
       " Row(origin_airport='LAX', destination_airport='LAS', total_flights=9594),\n",
       " Row(origin_airport='ORD', destination_airport='LGA', total_flights=9575),\n",
       " Row(origin_airport='SFO', destination_airport='JFK', total_flights=8440),\n",
       " Row(origin_airport='JFK', destination_airport='SFO', total_flights=8437)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    FROM flights\n",
    "        SELECT origin_airport, destination_airport, count(*) AS total_flights\n",
    "        GROUP BY origin_airport, destination_airport\n",
    "        ORDER BY total_flights DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You don't have to cap off SQL queries with `;`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note also:** The `take()` and `collect()` methods are actions at the end of spark queries which will **load the output data into the driver's memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the queried data using either SQL or DataFrame styles, and if we create a local `pandas` copy using `.toPandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>avg_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IAD</td>\n",
       "      <td>TTN</td>\n",
       "      <td>381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWF</td>\n",
       "      <td>PBI</td>\n",
       "      <td>260.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIC</td>\n",
       "      <td>CAE</td>\n",
       "      <td>228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RDU</td>\n",
       "      <td>IND</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10581</td>\n",
       "      <td>11618</td>\n",
       "      <td>163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FCA</td>\n",
       "      <td>MSO</td>\n",
       "      <td>148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SWF</td>\n",
       "      <td>RSW</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10581</td>\n",
       "      <td>12953</td>\n",
       "      <td>138.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14843</td>\n",
       "      <td>12264</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OAK</td>\n",
       "      <td>FLL</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11433</td>\n",
       "      <td>11423</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11433</td>\n",
       "      <td>13367</td>\n",
       "      <td>101.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14222</td>\n",
       "      <td>12173</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11423</td>\n",
       "      <td>11433</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10423</td>\n",
       "      <td>13487</td>\n",
       "      <td>90.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OKC</td>\n",
       "      <td>FLL</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>JAC</td>\n",
       "      <td>JFK</td>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DEN</td>\n",
       "      <td>PIA</td>\n",
       "      <td>84.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TTN</td>\n",
       "      <td>CVG</td>\n",
       "      <td>75.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MDT</td>\n",
       "      <td>DEN</td>\n",
       "      <td>72.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ORIGIN_AIRPORT DESTINATION_AIRPORT   avg_delay\n",
       "0             IAD                 TTN  381.000000\n",
       "1             SWF                 PBI  260.500000\n",
       "2             RIC                 CAE  228.000000\n",
       "3             RDU                 IND  208.000000\n",
       "4           10581               11618  163.000000\n",
       "5             FCA                 MSO  148.000000\n",
       "6             SWF                 RSW  140.000000\n",
       "7           10581               12953  138.333333\n",
       "8           14843               12264  122.000000\n",
       "9             OAK                 FLL  106.000000\n",
       "10          11433               11423  106.000000\n",
       "11          11433               13367  101.000000\n",
       "12          14222               12173  100.000000\n",
       "13          11423               11433  100.000000\n",
       "14          10423               13487   90.142857\n",
       "15            OKC                 FLL   90.000000\n",
       "16            JAC                 JFK   89.000000\n",
       "17            DEN                 PIA   84.750000\n",
       "18            TTN                 CVG   75.666667\n",
       "19            MDT                 DEN   72.500000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights\\\n",
    "    .select('FLIGHT_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', \n",
    "        'SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL',\n",
    "        'DEPARTURE_TIME', 'ARRIVAL_TIME', 'ARRIVAL_DELAY')\\\n",
    "    .groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\\\n",
    "    .avg('ARRIVAL_DELAY')\\\n",
    "    .withColumnRenamed('avg(ARRIVAL_DELAY)', 'avg_delay')\\\n",
    "    .sort(psf.desc('avg_delay'))\\\n",
    "    .limit(20)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more for luck..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_airport</th>\n",
       "      <th>destination_airport</th>\n",
       "      <th>airline</th>\n",
       "      <th>duration_hrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JFK</td>\n",
       "      <td>HNL</td>\n",
       "      <td>DL</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JFK</td>\n",
       "      <td>HNL</td>\n",
       "      <td>HA</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12478</td>\n",
       "      <td>12173</td>\n",
       "      <td>HA</td>\n",
       "      <td>10.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EWR</td>\n",
       "      <td>HNL</td>\n",
       "      <td>UA</td>\n",
       "      <td>10.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11618</td>\n",
       "      <td>12173</td>\n",
       "      <td>UA</td>\n",
       "      <td>10.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14735</th>\n",
       "      <td>CWA</td>\n",
       "      <td>ATW</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14736</th>\n",
       "      <td>MCO</td>\n",
       "      <td>HPN</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14737</th>\n",
       "      <td>GCC</td>\n",
       "      <td>RAP</td>\n",
       "      <td>OO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14738</th>\n",
       "      <td>FSD</td>\n",
       "      <td>LNK</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14739</th>\n",
       "      <td>SWF</td>\n",
       "      <td>LGA</td>\n",
       "      <td>EV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14740 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      origin_airport destination_airport airline  duration_hrs\n",
       "0                JFK                 HNL      DL         10.74\n",
       "1                JFK                 HNL      HA         10.58\n",
       "2              12478               12173      HA         10.46\n",
       "3                EWR                 HNL      UA         10.21\n",
       "4              11618               12173      UA         10.02\n",
       "...              ...                 ...     ...           ...\n",
       "14735            CWA                 ATW      EV           NaN\n",
       "14736            MCO                 HPN      EV           NaN\n",
       "14737            GCC                 RAP      OO           NaN\n",
       "14738            FSD                 LNK      EV           NaN\n",
       "14739            SWF                 LGA      EV           NaN\n",
       "\n",
       "[14740 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT origin_airport, destination_airport, airline, ROUND(AVG(air_time) / 60, 2) AS duration_hrs\n",
    "FROM flights\n",
    "GROUP BY origin_airport, destination_airport, airline\n",
    "ORDER BY duration_hrs DESC\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on immutability in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames are immutable, because you're supposed to leave the data as-is and do all the transformations in your code. If you wanted to update your DataFrame, you'd have to re-assign it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we needed to add an extra column. We could use the Spark DataFrame's `withColumn()` method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = flights.withColumn('duration_hrs', flights.AIR_TIME / 60)\n",
    "'duration_hrs' in flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter()` method takes on of two options:\n",
    "- A string, which represents what would follow a WHERE clause in SQL, or\n",
    "- A Spark Column of boolean (True/False) values\n",
    "\n",
    "In other words, the following two lines are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, AIRLINE: string, FLIGHT_NUMBER: int, TAIL_NUMBER: string, ORIGIN_AIRPORT: string, DESTINATION_AIRPORT: string, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, TAXI_OUT: int, WHEELS_OFF: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, AIR_TIME: int, DISTANCE: int, WHEELS_ON: int, TAXI_IN: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, CANCELLATION_REASON: string, AIR_SYSTEM_DELAY: int, SECURITY_DELAY: int, AIRLINE_DELAY: int, LATE_AIRCRAFT_DELAY: int, WEATHER_DELAY: int, duration_hrs: double] \n",
      "\n",
      "DataFrame[YEAR: int, MONTH: int, DAY: int, DAY_OF_WEEK: int, AIRLINE: string, FLIGHT_NUMBER: int, TAIL_NUMBER: string, ORIGIN_AIRPORT: string, DESTINATION_AIRPORT: string, SCHEDULED_DEPARTURE: int, DEPARTURE_TIME: int, DEPARTURE_DELAY: int, TAXI_OUT: int, WHEELS_OFF: int, SCHEDULED_TIME: int, ELAPSED_TIME: int, AIR_TIME: int, DISTANCE: int, WHEELS_ON: int, TAXI_IN: int, SCHEDULED_ARRIVAL: int, ARRIVAL_TIME: int, ARRIVAL_DELAY: int, DIVERTED: int, CANCELLED: int, CANCELLATION_REASON: string, AIR_SYSTEM_DELAY: int, SECURITY_DELAY: int, AIRLINE_DELAY: int, LATE_AIRCRAFT_DELAY: int, WEATHER_DELAY: int, duration_hrs: double]\n"
     ]
    }
   ],
   "source": [
    "print(flights.filter(\"air_time > 120\"), \"\\n\")\n",
    "print(flights.filter(flights.AIR_TIME > 120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** That when using string approach, the variable names are **not case sensitive**, just like with SQL queries. But when writing in Python code, they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select()` method takes multiple arguments that define which columns to select, either as strings of column names or the column objects themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, DEPARTURE_TIME: int]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"arrival_time\", flights.DEPARTURE_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note again:** That when selecting columns using strings, they are not case sensitive, just like in SQL. But passing a column object is case sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw just a moment above, you can perform operations on your columns to transform them using the `withColumn()` method, which returns all columns in the outputted DataFrame.\n",
    "\n",
    "But you can also do this inside `select()`, one of two ways:\n",
    "- Pass it a column object which we transform, with the option of renaming using the `alias()` method.\n",
    "- Pass it strings, which represent SQL expressions we'd pass to `SELECT`, with the option of renaming using `AS`. **But to do it this way, you have to use `selectExpr()` instead of `select()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, DEPARTURE_TIME: int, duration_hrs: double]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"arrival_time\", flights.DEPARTURE_TIME, (flights.AIR_TIME / 60).alias('duration_hrs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrival_time: int, departure_time: int, duration_hrs: double]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.selectExpr(\"arrival_time\", \"departure_time\", \"air_time / 60 AS duration_hrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[origin_airport: string, destination_airport: string, tail_number: string, avg_speed: double]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"origin_airport\", \"destination_airport\", \"tail_number\",\n",
    "               (flights.DISTANCE/(flights.AIR_TIME/60)).alias(\"avg_speed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[origin_airport: string, destination_airport: string, tail_number: string, avg_speed: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.selectExpr(\"origin_airport\", \"destination_airport\", \"tail_number\",\n",
    "                   \"distance/(air_time/60) as avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group our DataFrame using `groupBy()`. Once we've created a grouped DataFrame (GroupedData), we can use the GroupedDat methods for aggregation, like `min()`, `max()`, `count()`, `avg()`, and so on. We can aggregate the whole data by leaving `groupBy()` empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average flight duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| avg(duration_hrs)|\n",
      "+------------------+\n",
      "|1.8918604681687405|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .select((flights.AIR_TIME / 60).alias('duration_hrs'))\\\n",
    "    .groupBy()\\\n",
    "    .avg('duration_hrs')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average flight duration by airline carrier, sorted in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|airline| avg(duration_hrs)|\n",
      "+-------+------------------+\n",
      "|     VX| 3.043846601793797|\n",
      "|     UA|2.7472647618372408|\n",
      "|     AS|2.6307464073713316|\n",
      "|     B6| 2.397641535835225|\n",
      "|     AA|2.3301034689931734|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .selectExpr(\"airline\", \"air_time / 60 AS duration_hrs\")\\\n",
    "    .groupBy('airline')\\\n",
    "    .avg('duration_hrs')\\\n",
    "    .sort(psf.desc('avg(duration_hrs)'))\\\n",
    "    .show(5)\n",
    "\n",
    "# ^^ Note: See agg() below for renaming var at same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of flights taken, by airline and origin airport, sorted in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+\n",
      "|origin_airport|airline| count|\n",
      "+--------------+-------+------+\n",
      "|           ATL|     DL|221705|\n",
      "|           DFW|     AA|134270|\n",
      "|           MDW|     WN| 76350|\n",
      "|           LAS|     WN| 68520|\n",
      "|           BWI|     WN| 64063|\n",
      "+--------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .groupBy('origin_airport', 'airline')\\\n",
    "    .count()\\\n",
    "    .sort(psf.desc('count'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `agg()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the basic aggregation functions, there is also the generic `agg()` method, which lets us pass any of the aggregation functions from the `pyspark.sql.functions` submodule. (I loaded this submodule earlier as `psf`.)\n",
    "\n",
    "Every function in the submodule takes the name of a GroupedData column as its argument.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------------+\n",
      "|month|destination_airport|   dep_delay_stdev|\n",
      "+-----+-------------------+------------------+\n",
      "|    1|                ACY| 32.81265545745788|\n",
      "|    1|                EYW| 63.65052137951282|\n",
      "|    1|                OME|19.546923118225806|\n",
      "|    1|                RDM|34.260918828983726|\n",
      "|    1|                TWF| 7.985009333135624|\n",
      "+-----+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .groupBy('month', 'destination_airport')\\\n",
    "    .agg((psf.stddev('departure_delay')).alias('dep_delay_stdev'))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can use the `alias()` method with aggregation as well as when selecting columns as we saw earlier. But I don't think aliasing works when using the basic aggregators; you have to wrap it inside `agg()` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "+---------+--------------------+\n",
      "|       UA|United Air Lines ...|\n",
      "|       AA|American Airlines...|\n",
      "|       US|     US Airways Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "|       B6|     JetBlue Airways|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = spark.read.csv(\"./data/kaggle/airlines.csv\", inferSchema=True, header=True)\n",
    "airlines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|       CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|      ABE|Lehigh Valley Int...|  Allentown|   PA|    USA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|    Abilene|   TX|    USA|32.41132|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|Albuquerque|   NM|    USA|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|   Aberdeen|   SD|    USA|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|     Albany|   GA|    USA|31.53552| -84.19447|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports = spark.read.csv(\"./data/kaggle/airports.csv\", inferSchema=True, header=True)\n",
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `join()` method lets us do all kinds of joins. First arg is the dataset we want to use in our join, second arg `on` specifies the keys to join on, and third arg `how` specifies which kind of join to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "|iata_code|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|     duration_hrs|             AIRPORT|           CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "|      SEA|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                  5|          2354|            -11|      21|        15|           205|         194|     169|    1448|      404|      4|              430|         408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|2.816666666666667|Seattle-Tacoma In...|        Seattle|   WA|    USA|47.44898|-122.30931|\n",
      "|      PBI|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                 10|             2|             -8|      12|        14|           280|         279|     263|    2330|      737|      4|              750|         741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|4.383333333333334|Palm Beach Intern...|West Palm Beach|   FL|    USA|26.68316| -80.09559|\n",
      "|      CLT|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                 20|            18|             -2|      16|        34|           286|         293|     266|    2296|      800|     11|              806|         811|            5|       0|        0|               null|            null|          null|         null|               null|         null|4.433333333333334|Charlotte Douglas...|      Charlotte|   NC|    USA|35.21401| -80.94313|\n",
      "|      MIA|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                 20|            15|             -5|      15|        30|           285|         281|     258|    2342|      748|      8|              805|         756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|              4.3|Miami Internation...|          Miami|   FL|    USA|25.79325| -80.29056|\n",
      "|      ANC|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                 25|            24|             -1|      11|        35|           235|         215|     199|    1448|      254|      5|              320|         259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|3.316666666666667|Ted Stevens Ancho...|      Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "+---------+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+-----------------+--------------------+---------------+-----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights\\\n",
    "    .withColumnRenamed(\"destination_airport\", \"iata_code\")\\\n",
    "    .join(airports, on=\"iata_code\", how=\"leftouter\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising and summarising data with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't use traditional plotting packages directly with Spark DataFrames. There are three methods to plot data from Spark DataFrames:\n",
    "1. pyspark_dist_explore methods\n",
    "2. toPandas() method to create a pandas dataframe\n",
    "3. HandySpark library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are methods to compute basic summaries of Spark DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      arrival_time|\n",
      "+-------+------------------+\n",
      "|  count|           5726566|\n",
      "|   mean|1476.4911879126164|\n",
      "| stddev|  526.319737212267|\n",
      "|    min|                 1|\n",
      "|    25%|              1059|\n",
      "|    50%|              1513|\n",
      "|    75%|              1917|\n",
      "|    max|              2400|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.select('arrival_time').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the slight less detailed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      arrival_time|\n",
      "+-------+------------------+\n",
      "|  count|           5726566|\n",
      "|   mean|1476.4911879126164|\n",
      "| stddev|  526.319737212267|\n",
      "|    min|                 1|\n",
      "|    max|              2400|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.select('arrival_time').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE TO SELF: Add sections on pyspark_dist_explore and HandySpark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-play",
   "language": "python",
   "name": "spark-play"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
